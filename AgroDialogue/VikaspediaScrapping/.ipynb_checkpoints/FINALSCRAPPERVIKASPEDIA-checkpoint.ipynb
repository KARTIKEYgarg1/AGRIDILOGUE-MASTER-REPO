{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b901deef-31b9-4b3c-8746-8330bb969b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "664b697a-d7d6-4a03-8286-39144627a14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL=\"https://vikaspedia.in\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc261593-967f-4170-859b-ed5b69ae9fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_name(folder_name):\n",
    "    # Replace special characters in folder name with '-'\n",
    "    # For special cases like while scraping data for category: Guidelines for Release/Notification, Provisional Notification and De-notification of Cultivars\n",
    "    cleaned_name = re.sub(r'[\\/:*?\"<>|]', '-', folder_name)\n",
    "    return cleaned_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbc28034-066c-438f-b19e-880bab8831c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchData(url):\n",
    "    dat=requests.get(url,verify=False)\n",
    "    soup=BeautifulSoup(dat.text,\"html.parser\")\n",
    "    MiddleColumn = soup.find('div', id='MiddleColumn_internal')\n",
    "    # Remove all <a> tags from MiddleColumn\n",
    "    for a_tag in MiddleColumn.find_all('a'):\n",
    "        a_tag.decompose()\n",
    "    # Remove all tags and add only text\n",
    "    text_content = ''\n",
    "    for element in MiddleColumn.contents:\n",
    "        if element.name == 'h3' or element.name == 'h4' or element.name == 'p':\n",
    "            text_content += str(element.text) + '\\n'\n",
    "        else:\n",
    "            text_content += str(element)\n",
    "    # Remove HTML tags using regular expression\n",
    "    return re.sub(r'<.*?>', '', text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d23ee641-e674-426a-83b1-765dbabefcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def webScrapeSubCategory(url, folder_path='.'):\n",
    "    #For Delay\n",
    "    try:\n",
    "        time.sleep(10)\n",
    "        data = requests.get(url, verify=False) #Ignoring SSL certificate verification\n",
    "        soup = BeautifulSoup(data.text, \"html.parser\") # Generating Soup\n",
    "        a_tags = soup.find_all('a', class_='folderfile_name') #Fetching a tags\n",
    "        result = []\n",
    "        if a_tags:\n",
    "            for tag in a_tags:\n",
    "                category_name = tag.text.strip() #removing extra spaces or lines\n",
    "                category_folder_name = clean_name(category_name) #clean name\n",
    "                category_folder_path = os.path.join(folder_path, category_folder_name) # calulate path\n",
    "                os.makedirs(category_folder_path, exist_ok=True) # If directory does not exitst create one\n",
    "                print(f\"Scraping data for category: {category_name} {url}\") #Show message --> For Debugging\n",
    "                category_url = BASE_URL + tag.get('href') #Next website\n",
    "                subcategories = webScrapeSubCategory(category_url, category_folder_path) # recursively call\n",
    "                result.append({category_name: subcategories})\n",
    "        else:\n",
    "            title = soup.find('h3', class_='card-title title')\n",
    "            scrapped_data = title.text + \"\\n\\n\" + fetchData(url)\n",
    "            with open(os.path.join(folder_path, clean_name(title.text)+'.txt'), 'w', encoding='utf-8') as file:\n",
    "                file.write(scrapped_data)\n",
    "            return scrapped_data\n",
    "    \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(\"EXCEPTION OCCURRED\",e)\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acf3fa7e-2ad4-47d6-a72e-f7a7e0c54503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def webScrape(url, start,end,folder_path='.'):\n",
    "    #For Delay\n",
    "    try:\n",
    "        time.sleep(10)\n",
    "        data = requests.get(url, verify=False) #Ignoring SSL certificate verification\n",
    "        soup = BeautifulSoup(data.text, \"html.parser\") # Generating Soup\n",
    "        a_tags = soup.find_all('a', class_='folderfile_name') #Fetching a tags\n",
    "        result = []\n",
    "        if a_tags:\n",
    "            for tag in a_tags[start:end+1]:\n",
    "                category_name = tag.text.strip() #removing extra spaces or lines\n",
    "                category_folder_name = clean_name(category_name) #clean name\n",
    "                category_folder_path = os.path.join(folder_path, category_folder_name) # calulate path\n",
    "                os.makedirs(category_folder_path, exist_ok=True) # If directory does not exitst create one\n",
    "                print(f\"Scraping data for category: {category_name} {url}\") #Show message --> For Debugging\n",
    "                category_url = BASE_URL + tag.get('href') #Next website\n",
    "                subcategories = webScrapeSubCategory(category_url, category_folder_path) # recursively call\n",
    "                result.append({category_name: subcategories})\n",
    "        else:\n",
    "            title = soup.find('h3', class_='card-title title')\n",
    "            scrapped_data = title.text + \"\\n\\n\" + fetchData(url)\n",
    "            with open(os.path.join(folder_path, clean_name(title.text)+'.txt'), 'w', encoding='utf-8') as file:\n",
    "                file.write(scrapped_data)\n",
    "            return scrapped_data\n",
    "    \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(\"EXCEPTION OCCURRED\",e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc529057-989b-4750-a707-3512fb1991a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\9616k\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vikaspedia.in'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Agri Credit', 0], ['Agri Directory', 1], ['Agri Exports', 2], ['Agri Inputs', 3], ['Agri Insurance', 4], ['Agro enterprises', 5], ['Best Practices', 6], ['Crop Production', 7], ['Fisheries', 8], ['Forestry', 9], ['ICT applications in Agriculture', 10], ['Livestock', 11], ['Market information', 12], ['National Schemes for Farmers', 13], ['Policies and Schemes', 14], ['Post Harvest Technologies', 15], ['Poultry', 16], ['State-specific schemes for farmers', 17], ['Women and agriculture', 18]]\n"
     ]
    }
   ],
   "source": [
    "#For generating list with indices\n",
    "data = requests.get(BASE_URL+\"/agriculture\", verify=False) #Ignoring SSL certificate verification\n",
    "soup = BeautifulSoup(data.text, \"html.parser\") # Generating Soup\n",
    "a_tags = soup.find_all('a', class_='folderfile_name') #Fetching a tags\n",
    "result = []\n",
    "if a_tags:\n",
    "    for i,tag in enumerate(a_tags):\n",
    "        result.append([tag.text,i])\n",
    "    print(result)\n",
    "else:\n",
    "    print(\"No a tags found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056aeccc-f3f9-4ff0-8d66-26ea211c64d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\9616k\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vikaspedia.in'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for category: Poultry https://vikaspedia.in/agriculture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\9616k\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vikaspedia.in'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION OCCURRED\n",
      "Scraping data for category: State-specific schemes for farmers https://vikaspedia.in/agriculture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\9616k\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vikaspedia.in'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for category: Andaman and Nicobar https://vikaspedia.in/agriculture/state-specific-schemes-for-farmers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\9616k\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vikaspedia.in'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for category: Agricultural Credit https://vikaspedia.in/agriculture/state-specific-schemes-for-farmers/andaman-and-nicobar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\9616k\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vikaspedia.in'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\9616k\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vikaspedia.in'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for category: Agricultural Marketing https://vikaspedia.in/agriculture/state-specific-schemes-for-farmers/andaman-and-nicobar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\9616k\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vikaspedia.in'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "allSectors=webScrape(BASE_URL+\"/agriculture\",16,18,\"VIKASPEDIA_DATA\") #START AND END ARE BOTH INCLUDING INDICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d23066d-8fef-422e-afb5-ed7241816081",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
