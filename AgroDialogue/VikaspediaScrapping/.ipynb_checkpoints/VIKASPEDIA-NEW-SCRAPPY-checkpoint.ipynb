{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c060c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:10: SyntaxWarning: invalid escape sequence '\\P'\n",
      "<>:10: SyntaxWarning: invalid escape sequence '\\P'\n",
      "C:\\Users\\9616k\\AppData\\Local\\Temp\\ipykernel_17116\\1646112386.py:10: SyntaxWarning: invalid escape sequence '\\P'\n",
      "  path=\"C:\\Program Files (x86)\\msedgedriver.exe\"\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import os\n",
    "path=\"C:\\Program Files (x86)\\msedgedriver.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7049b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL=\"https://xn--clcu6av0at5becfj8m.xn--xkc2dl3a5ee0h\"\n",
    "EXTENSION=\"/energy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e061de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_name(folder_name):\n",
    "    # Replace special characters in folder name with '-'\n",
    "    # For special cases like while scraping data for category: Guidelines for Release/Notification, Provisional Notification and De-notification of Cultivars\n",
    "    cleaned_name = re.sub(r'[\\/:*?\"<>| ]', '-', folder_name)\n",
    "    cleaned_name = cleaned_name.replace(\"\\\"\", \"\")\n",
    "    cleaned_name = cleaned_name.replace(\"'\", \"\")\n",
    "    return cleaned_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8a4b789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchData(soup):\n",
    "    x=0\n",
    "    MiddleColumn = soup.find('div', id='MiddleColumn')\n",
    "    MiddleColumn = soup.find('div', id='texttospeak')\n",
    "    if MiddleColumn:\n",
    "        # Remove all tags and add only text\n",
    "        text_content = ''\n",
    "        for element in MiddleColumn.contents:\n",
    "            x+=1\n",
    "            if element.name == 'h3' or element.name == 'h4' or element.name == 'p':\n",
    "                text_content += str(element.text) + '\\n'\n",
    "            else:\n",
    "                text_content += str(element)\n",
    "    if x<3:\n",
    "        MiddleColumn = soup.find('div', id='MiddleColumn_internal')\n",
    "        # Remove all <a> tags from MiddleColumn\n",
    "        for a_tag in MiddleColumn.find_all('a'):\n",
    "            a_tag.decompose()\n",
    "        # Remove all tags and add only text\n",
    "        text_content = ''\n",
    "        for element in MiddleColumn.contents:\n",
    "            if element.name == 'h3' or element.name == 'h4' or element.name == 'p':\n",
    "                text_content += str(element.text) + '\\n'\n",
    "            else:\n",
    "                text_content += str(element)\n",
    "    # Remove HTML tags using regular expression\n",
    "    return re.sub(r'<.*?>', '', text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9f6e0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def webScrapeSubCategory(url, folder_path='.'):\n",
    "    #For Delay\n",
    "    try:\n",
    "        data = requests.get(url, verify=False) #Ignoring SSL certificate verification\n",
    "        soup = BeautifulSoup(data.text, \"html.parser\") # Generating Soup\n",
    "#         print(soup)\n",
    "        a_tags = soup.find_all('a', class_='folderfile_name') #Fetching a tags\n",
    "        result = []\n",
    "        if a_tags:\n",
    "            for tag in a_tags:\n",
    "                category_name = tag.text.strip() #removing extra spaces or lines\n",
    "                category_folder_name = clean_name(category_name) #clean name\n",
    "                category_folder_path = os.path.join(folder_path, category_folder_name) # calulate path\n",
    "                os.makedirs(category_folder_path, exist_ok=True) # If directory does not exitst create one\n",
    "                print(f\"Scraping data for category: {category_name} {url}\") #Show message --> For Debugging\n",
    "                category_url = BASE_URL + tag.get('href') #Next website\n",
    "                subcategories = webScrapeSubCategory(category_url, category_folder_path) # recursively call\n",
    "                result.append({category_name: subcategories})\n",
    "        else:\n",
    "            print(f\"Fetching content for {url}\")\n",
    "            driver = webdriver.Edge() \n",
    "            driver.get(url)\n",
    "            time.sleep(5)\n",
    "            html_content = driver.page_source\n",
    "            driver.quit() \n",
    "            soup = BeautifulSoup(html_content, \"html.parser\") # Generating Soup\n",
    "            title = soup.find('h3', class_='title')\n",
    "            if title is None:\n",
    "                title = soup.find('h3')\n",
    "            scrapped_data = title.text + \"\\n\\n\" + fetchData(soup)     \n",
    "            with open(os.path.join(folder_path, clean_name(title.text)+'.txt'), 'w', encoding='utf-8') as file:\n",
    "                file.write(scrapped_data)\n",
    "            return scrapped_data\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(\"EXCEPTION OCCURRED\",e)\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27cb0097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def webScrape(url, start,end,folder_path='.'):\n",
    "    #For Delay\n",
    "    try:\n",
    "        data = requests.get(url, verify=False) #Ignoring SSL certificate verification\n",
    "        soup = BeautifulSoup(data.text, \"html.parser\") # Generating Soup\n",
    "        a_tags = soup.find_all('a', class_='folderfile_name') #Fetching a tags\n",
    "        result = []\n",
    "        if a_tags:\n",
    "            for tag in a_tags:\n",
    "                category_name = tag.text.strip() #removing extra spaces or lines\n",
    "                category_folder_name = clean_name(category_name) #clean name\n",
    "                category_folder_path = os.path.join(folder_path, category_folder_name) # calulate path\n",
    "                os.makedirs(category_folder_path, exist_ok=True) # If directory does not exitst create one\n",
    "                print(f\"Scraping data for category: {category_name} {url}\") #Show message --> For Debugging\n",
    "                category_url = BASE_URL + tag.get('href') #Next website\n",
    "                subcategories = webScrapeSubCategory(category_url, category_folder_path) # recursively call\n",
    "                result.append({category_name: subcategories})\n",
    "        else:\n",
    "            title = soup.find('h3', class_='card-title title')\n",
    "            scrapped_data = title.text + \"\\n\\n\" + fetchData(url)\n",
    "            with open(os.path.join(folder_path, clean_name(title.text)+'.txt'), 'w', encoding='utf-8') as file:\n",
    "                file.write(scrapped_data)\n",
    "            return scrapped_data\n",
    "    \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(\"EXCEPTION OCCURRED\",e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19249df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['அரசு திட்டங்களும் கொள்கைகளும்', 0], ['எரிசக்தி அடிப்படை', 1], ['எரிசக்தி உற்பத்தி', 2], ['எரிசக்தி சேமிப்பு', 3], ['எரிசக்தி திறன்', 4], ['சிறந்த செயல்முறைகள்', 5], ['சுற்றுச்சூழல்', 6], ['தொிந்து கொள்ள வேண்டிய எரிசக்தி சார்ந்த தகவல்கள்', 7], ['பெண்களும் எரிசக்தியும்', 8]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\9616k\\Documents\\CODES\\WebScrapLearning\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host 'xn--clcu6av0at5becfj8m.xn--xkc2dl3a5ee0h'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#For generating list with indices\n",
    "data = requests.get(BASE_URL+EXTENSION, verify=False) #Ignoring SSL certificate verification\n",
    "soup = BeautifulSoup(data.text, \"html.parser\") # Generating Soup\n",
    "a_tags = soup.find_all('a', class_='folderfile_name') #Fetching a tags\n",
    "# print(soup)\n",
    "result = []\n",
    "if a_tags:\n",
    "    for i,tag in enumerate(a_tags):\n",
    "        result.append([tag.text,i])\n",
    "    print(result)\n",
    "else:\n",
    "    print(\"No a tags found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b87762",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\9616k\\Documents\\CODES\\WebScrapLearning\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host 'xn--clcu6av0at5becfj8m.xn--xkc2dl3a5ee0h'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for category: அரசு திட்டங்களும் கொள்கைகளும் https://xn--clcu6av0at5becfj8m.xn--xkc2dl3a5ee0h/energy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\9616k\\Documents\\CODES\\WebScrapLearning\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host 'xn--clcu6av0at5becfj8m.xn--xkc2dl3a5ee0h'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching content for https://xn--clcu6av0at5becfj8m.xn--xkc2dl3a5ee0h/energy/அரசு-திட்டங்களும்-கொள்கைகளும்\n",
      "Scraping data for category: எரிசக்தி அடிப்படை https://xn--clcu6av0at5becfj8m.xn--xkc2dl3a5ee0h/energy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\9616k\\Documents\\CODES\\WebScrapLearning\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host 'xn--clcu6av0at5becfj8m.xn--xkc2dl3a5ee0h'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching content for https://xn--clcu6av0at5becfj8m.xn--xkc2dl3a5ee0h/energy/எரிசக்தி-அடிப்படை\n",
      "Scraping data for category: எரிசக்தி உற்பத்தி https://xn--clcu6av0at5becfj8m.xn--xkc2dl3a5ee0h/energy\n"
     ]
    }
   ],
   "source": [
    "allSectors=webScrape(BASE_URL+EXTENSION,0,0,\"Tamil/energy\") #START AND END ARE BOTH INCLUDING INDICES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d104b5",
   "metadata": {},
   "source": [
    "# FLATTEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "228ee324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e47f1fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move(source,dest):\n",
    "    isExist = os.path.exists(source)\n",
    "    os.makedirs(dest, exist_ok=True)\n",
    "    if isExist is False:\n",
    "        return \"PATH DOES NOT  EXIST\"\n",
    "    try:\n",
    "        for item in os.listdir(source):\n",
    "            item_path = os.path.join(source, item)\n",
    "            if os.path.isdir(item_path):\n",
    "                move(item_path,dest)  # Recursively call move function for directories\n",
    "            else:\n",
    "                # print(item)\n",
    "                with open(os.path.join(os.getcwd(),item_path),\"r\",encoding=\"utf-8\") as c:\n",
    "                    with open(os.path.join(os.getcwd(),dest,clean_name(item)),\"w\",encoding=\"utf-8\") as nf:\n",
    "                        nf.write(c.read())\n",
    "                # print(f\"SUCCESS FOR {item}\")\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5482d05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGRICULTURE\n",
      "e-governance\n",
      "health\n"
     ]
    }
   ],
   "source": [
    "source=\"SINDHI\"\n",
    "for category in os.listdir(source):\n",
    "    print(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9e0b3e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "source=\"SINDHI/health\"\n",
    "dest=source+\"_FLATTEND\"\n",
    "# dest=\"VIKASPEDIA_DATA_SOCIAL_WELFARE_FLATTENED\"\n",
    "for item in os.listdir(source):\n",
    "    item_path = os.path.join(source, item)\n",
    "    if os.path.isdir(item_path):\n",
    "            move(item_path, os.path.join(dest, item))  # Recursively call move function for directories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d6d4c1",
   "metadata": {},
   "source": [
    "# INSIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "116f58d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "Total Folders: 0\n",
      "Total Empty Folders: 0\n",
      "Total Files: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "source = \"DOHNGRI\"\n",
    "tot_folders = 0\n",
    "tot_files = 0\n",
    "tot_empty = 0\n",
    "\n",
    "for item in os.listdir(source):\n",
    "    folder_path = os.path.join(source, item)\n",
    "    if item.endswith(\"_FLATTENED\") and os.path.isdir(folder_path):\n",
    "        sub_folders = os.listdir(folder_path)\n",
    "        tot_folders += len(sub_folders)  # Accumulate total sub-folders\n",
    "        \n",
    "        # Count files in each folder\n",
    "        for sub_folder in sub_folders:\n",
    "            sub_folder_path = os.path.join(folder_path, sub_folder)\n",
    "            if os.path.isdir(sub_folder_path):\n",
    "                files_in_sub_folder = os.listdir(sub_folder_path)\n",
    "                if not files_in_sub_folder:\n",
    "                    tot_empty += 1  # Increment empty folder count\n",
    "                tot_files += len(files_in_sub_folder)  # Accumulate total files\n",
    "                print(sub_folder, len(files_in_sub_folder))\n",
    "\n",
    "print(\"----------------------------------\")\n",
    "print(\"Total Folders:\", tot_folders)\n",
    "print(\"Total Empty Folders:\", tot_empty)\n",
    "print(\"Total Files:\", tot_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9ebe692a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "এগ্রি-ইন্সুরেন্স 1\n",
      "এগ্রি-ক্রেদিট 0\n",
      "এগ্রী-ইনপুটশীং 3\n",
      "পোলিসিশীং-অমসুং-স্কিমশীং 6\n",
      "মার্কেটকী-ইনফোর্মেসন 2\n",
      "লাইভষ্টোক 0\n",
      "VLEশীংগী-রিসোর্সশীং 2\n",
      "ই-গভর্নান্স-সর্ভিসেস-ওনলাইন 1\n",
      "দিজিটেল-ইন্দিয়া 0\n",
      "দিজিটেল-পেমেন্ট 5\n",
      "নাগরিক্কী-ফিদবেক্কীদমক্তা-মোবাইল-এপশীং 0\n",
      "ফার্মা-জন-সমাধান-(ই-গবর্নান্স) 1\n",
      "ভারতকী-জুদিসিএল-সর্ভিসশীং 1\n",
      "ভারত্তা-ই-গভর্নান্স 2\n",
      "মোবাইল-গবর্নান্স 0\n",
      "সিটিজেন-এক্সেস-তু-ইনফোর্মেসন-(RTI) 2\n",
      "সিটিজেনগী-সর্ভিসশীং 2\n",
      "অঙাংগী-হকশীং 3\n",
      "অঙাংশীংগী-কোর্নর 2\n",
      "কেরিয়র-গাইদেন্স 1\n",
      "দিজিটেল-লিটরেসি 10\n",
      "পেরেন্টশীংগী-কোর্ণর 1\n",
      "পোলিসিশীং-অমসুং-স্কিমশীং 4\n",
      "এনভাইরনমেন্ট 1\n",
      "এনর্জি-এক্সেস 2\n",
      "এনর্জি-য়োকপা 0\n",
      "এনর্জিগী-বেসিকশীং 1\n",
      "পোলিসি-সপোর্ট 3\n",
      "আয়ুশ 1\n",
      "এজিং(অহল-ওইরকপা) 0\n",
      "দিজিজ-অমসুং-দিসওর্দরশীং 0\n",
      "নুপীগী-হকশেল 5\n",
      "ন্যুত্রিসন 1\n",
      "পুন্সিগী-ওইবা-অচুম্বশীং 14\n",
      "পোলিসিশীং-অমসুং-স্কিমশীং 5\n",
      "ফার্ষ্ট-এইদ 6\n",
      "সেনিটেসন-অমসুং-হায়জিন 4\n",
      "হকশেলগী-কেম্পেইনশীং 1\n",
      "হেলথ-কেয়র-ইনোভেসনশীং 1\n",
      "অর্বান-পোবর্টি-এলেভিএসন 0\n",
      "এন্ত্রিপ্রেন্যুওরশিপ 3\n",
      "ওর্গানাইজ-তৌদবা-সেক্টর-ৱেলফেয়র 6\n",
      "খুঙ্গংগী-লায়রবা-কোকহনবা 8\n",
      "দিজাষ্টর-মেনেজমেন্ট 15\n",
      "নুপী-অমদি-অঙাংশীংগী-চাউখৎ-থৌরাং-পায়খৎপা 13\n",
      "ফাইনান্সিএল-ইনক্লুজন 3\n",
      "মীমান্নদ্রবা-মীওইশীংগী-য়াইফ-থৌরাং 2\n",
      "সমাজগী-ফত্তবশীংগা-লান্থেংনবা 5\n",
      "সেদ্যুল-কাষ্টকী-য়াইফ-থৌরাং 1\n",
      "সেদ্যুল-ত্রাইবশীংগী-য়াইফ-থৌরাং 3\n",
      "সেনিয়র-সিটিজেনশীংগী-য়াইফ-থৌরাং 5\n",
      "সোসিএল-সেক্যুরিটি 3\n",
      "স্কিল-দিভলপমেন্ট 5\n",
      "----------------------------------\n",
      "Total Folders: 14\n",
      "Total Empty Folders: 9\n",
      "Total Files: 166\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "source = \"MANIPURI\"\n",
    "tot_folders = 0\n",
    "tot_files = 0\n",
    "tot_empy = 0\n",
    "\n",
    "for item in os.listdir(source):\n",
    "    folder_path = os.path.join(source, item)\n",
    "    if item.endswith(\"_FLATTEND\") and os.path.isdir(folder_path):\n",
    "        sub_folders = os.listdir(folder_path)\n",
    "        tot_folders=len(sub_folders)\n",
    "        # Count files in each folder\n",
    "        for i in sub_folders:\n",
    "            x=len(os.listdir(os.path.join(source,item,i)))\n",
    "            if x==0:\n",
    "                tot_empy+=1\n",
    "            else:\n",
    "                tot_files+=x\n",
    "            print(i,x)\n",
    "print(\"----------------------------------\")\n",
    "print(\"Total Folders:\", tot_folders)\n",
    "print(\"Total Empty Folders:\", tot_empy)\n",
    "print(\"Total Files:\", tot_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "061e2041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ऐगरी-बिमो 1\n",
      "कोकिड 0\n",
      "चौपायो-माल 0\n",
      "निती-ऐं-रिथाउं 1\n",
      "पोख-इंपुट 0\n",
      "मच्छी-पालन 0\n",
      "डि॒जिटल-माली-सेवायूं 1\n",
      "आयूश 0\n",
      "खोराक 0\n",
      "जाईफाणी-सिहत 0\n",
      "बा॒राणा-हक 0\n",
      "बिमारी 18\n",
      "Total Folders: 5\n",
      "Total Empty Folders: 8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "source = \"SINDHI\"\n",
    "tot_folders = 0\n",
    "tot_empy = 0\n",
    "\n",
    "for item in os.listdir(source):\n",
    "    folder_path = os.path.join(source, item)\n",
    "    if item.endswith(\"_FLATTEND\") and os.path.isdir(folder_path):\n",
    "        sub_folders = os.listdir(folder_path)\n",
    "        tot_folders=len(sub_folders)\n",
    "        # Count files in each folder\n",
    "        for i in sub_folders:\n",
    "            x=len(os.listdir(os.path.join(source,item,i)))\n",
    "            if x==0:\n",
    "                tot_empy+=1\n",
    "            print(i,x)\n",
    "print(\"Total Folders:\", tot_folders)\n",
    "print(\"Total Empty Folders:\", tot_empy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896c75ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
